{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bithoseoconda264274678644477794f47fddcf829f7c",
   "display_name": "Python 3.6.10 64-bit ('hoseo': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as  tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['bug', 'dragon', 'electricity', 'esp', 'evil', 'fairy', 'fight', 'fire', 'ghost', 'grass', 'ice', 'land', 'normal', 'posion', 'rock', 'steel', 'water']\nTotal number of categories: 17\nTotal number of images in dataset: 55588\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# folder_path = \"D:\\yun\\yun-python\\딥러닝(kmooc 연습)\\포켓몬 이미지\"\n",
    "folder_path = \"D:\\yun\\yun-python\\deeplearning\\pokemon-train-image\"\n",
    "folder_list = os.listdir(folder_path)\n",
    "print(folder_list)\n",
    "print(f'Total number of categories: {len(folder_list)}')\n",
    "\n",
    "counts = {}\n",
    "for c in folder_list:\n",
    "    counts[c] = len(os.listdir(os.path.join(folder_path, c)))\n",
    "    \n",
    "print(f'Total number of images in dataset: {sum(list(counts.values()))}')\n",
    "\n",
    "# categories=['강철', '격투', '고스트', '노멀', '독', '드래곤', '땅', '물', '바위', '벌레', '불꽃', '악', '얼음', '에스퍼', '전기', '페어리', '풀']\n",
    "\n",
    "categories=['bug', 'dragon', 'electricity', 'esp', 'evil', 'fairy', 'fight', 'fire', 'ghost', 'grass', 'ice', 'land', 'normal', 'posion', 'rock', 'steel', 'water']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('water', 6410), ('normal', 5096), ('fire', 4101), ('bug', 4001), ('grass', 3916), ('land', 3448), ('esp', 3177), ('posion', 3086), ('electricity', 3046), ('rock', 2905), ('fight', 2904), ('ice', 2670), ('dragon', 2397), ('ghost', 2356), ('steel', 2305), ('evil', 2072), ('fairy', 1698)]\n"
     ]
    }
   ],
   "source": [
    "imbalanced = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "print(imbalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caltech_dir = \"..\\deeplearning\\pokemon-image\"\n",
    "caltech_dir = \"pokemon-train-image\"\n",
    "categories=['bug', 'dragon', 'electricity', 'esp', 'evil', 'fairy', 'fight', 'fire', 'ghost', 'grass', 'ice', 'land', 'normal', 'posion', 'rock', 'steel', 'water']\n",
    "nb_classes = len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !dir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 55588 images belonging to 17 classes.\n",
      "Found 55588 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "# 이미지 전처리\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "      rotation_range=10,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=False,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        # 타깃 디렉터리\n",
    "        caltech_dir,\n",
    "        # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range = 30,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.1,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=False,\n",
    "                                 fill_mode='nearest')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "                                          caltech_dir,\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size=20,\n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "categorical\n"
     ]
    }
   ],
   "source": [
    "class_names = generator.class_mode\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3,3), padding='same', input_shape=(150,150,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_9 (Conv2D)            (None, 150, 150, 64)      1792      \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 75, 75, 64)        0         \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 75, 75, 64)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 75, 75, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 37, 37, 128)       0         \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 37, 37, 128)       0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 37, 37, 256)       295168    \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 18, 18, 256)       0         \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 18, 18, 256)       0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 82944)             0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 256)               21233920  \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 17)                4369      \n=================================================================\nTotal params: 21,609,105\nTrainable params: 21,609,105\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Conv2D(64, kernel_size=(3,3),padding='same', input_shape=(150,150,3), activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv2D(64, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv2D(128, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Conv2D(256, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(256, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 150, 150, 64)      1792      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 75, 75, 64)        0         \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 75, 75, 64)        256       \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 75, 75, 64)        0         \n_________________________________________________________________\nconv2d_5 (Conv2D)            (None, 75, 75, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 37, 37, 64)        0         \n_________________________________________________________________\nbatch_normalization_5 (Batch (None, 37, 37, 64)        256       \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 37, 37, 64)        0         \n_________________________________________________________________\nconv2d_6 (Conv2D)            (None, 37, 37, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 18, 18, 128)       0         \n_________________________________________________________________\nbatch_normalization_6 (Batch (None, 18, 18, 128)       512       \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 18, 18, 128)       0         \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 18, 18, 256)       295168    \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 9, 9, 256)         0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 9, 9, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 20736)             0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 256)               5308672   \n_________________________________________________________________\nbatch_normalization_7 (Batch (None, 256)               1024      \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 17)                4369      \n=================================================================\nTotal params: 5,722,833\nTrainable params: 5,721,809\nNon-trainable params: 1,024\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 설정\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 55588 images belonging to 17 classes.\n",
      "Found 55588 images belonging to 17 classes.\n"
     ]
    }
   ],
   "source": [
    "# 이미지 전처리\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      vertical_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "        # 타깃 디렉터리\n",
    "        caltech_dir,\n",
    "        # 모든 이미지를 150 × 150 크기로 바꿉니다\n",
    "        target_size=(150, 150),\n",
    "        batch_size=40,\n",
    "        shuffle=True,\n",
    "        class_mode='categorical')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                 rotation_range = 40,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 height_shift_range=0.2,\n",
    "                                 shear_range=0.2,\n",
    "                                 zoom_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True,\n",
    "                                 fill_mode='nearest')\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "                                          caltech_dir,\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size=40,\n",
    "                                          class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(32, kernel_size=(3,3),padding='same', input_shape=(150,150,3), activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Conv2D(64, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Conv2D(128, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Conv2D(256, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model3.add(Dropout(0.2))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(256, activation='relu'))\n",
    "model3.add(BatchNormalization())\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습 설정\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3s/step - loss: 2.3202 - accuracy: 0.2875 - val_loss: 2.3520 - val_accuracy: 0.2665\n",
      "Epoch 14/150\n",
      "150/150 [==============================] - 500s 3s/step - loss: 2.3098 - accuracy: 0.2805 - val_loss: 2.3434 - val_accuracy: 0.2690\n",
      "Epoch 15/150\n",
      "150/150 [==============================] - 501s 3s/step - loss: 2.3017 - accuracy: 0.2912 - val_loss: 2.3043 - val_accuracy: 0.2835\n",
      "Epoch 16/150\n",
      "150/150 [==============================] - 509s 3s/step - loss: 2.3426 - accuracy: 0.2698 - val_loss: 2.4429 - val_accuracy: 0.2465\n",
      "Epoch 17/150\n",
      "150/150 [==============================] - 500s 3s/step - loss: 2.2897 - accuracy: 0.2935 - val_loss: 3.2812 - val_accuracy: 0.1410\n",
      "Epoch 18/150\n",
      "150/150 [==============================] - 513s 3s/step - loss: 2.2673 - accuracy: 0.2971 - val_loss: 2.4199 - val_accuracy: 0.2650\n",
      "Epoch 19/150\n",
      "150/150 [==============================] - 517s 3s/step - loss: 2.2987 - accuracy: 0.2842 - val_loss: 2.3310 - val_accuracy: 0.2770\n",
      "Epoch 20/150\n",
      "150/150 [==============================] - 521s 3s/step - loss: 2.2581 - accuracy: 0.2967 - val_loss: 2.2877 - val_accuracy: 0.2695\n",
      "Epoch 21/150\n",
      "150/150 [==============================] - 518s 3s/step - loss: 2.2626 - accuracy: 0.2958 - val_loss: 2.2496 - val_accuracy: 0.2860\n",
      "Epoch 22/150\n",
      "150/150 [==============================] - 519s 3s/step - loss: 2.2740 - accuracy: 0.2885 - val_loss: 2.3736 - val_accuracy: 0.2535\n",
      "Epoch 23/150\n",
      "150/150 [==============================] - 518s 3s/step - loss: 2.2545 - accuracy: 0.2965 - val_loss: 2.2243 - val_accuracy: 0.3075\n",
      "Epoch 24/150\n",
      "150/150 [==============================] - 523s 3s/step - loss: 2.2403 - accuracy: 0.2968 - val_loss: 2.3756 - val_accuracy: 0.2755\n",
      "Epoch 25/150\n",
      "150/150 [==============================] - 570s 4s/step - loss: 2.2469 - accuracy: 0.2992 - val_loss: 2.3529 - val_accuracy: 0.2835\n",
      "Epoch 26/150\n",
      "150/150 [==============================] - 509s 3s/step - loss: 2.2229 - accuracy: 0.3063 - val_loss: 2.1969 - val_accuracy: 0.3085\n",
      "Epoch 27/150\n",
      "150/150 [==============================] - 513s 3s/step - loss: 2.2075 - accuracy: 0.3052 - val_loss: 2.1963 - val_accuracy: 0.3110\n",
      "Epoch 28/150\n",
      "150/150 [==============================] - 532s 4s/step - loss: 2.1974 - accuracy: 0.3188 - val_loss: 2.2605 - val_accuracy: 0.3020\n",
      "Epoch 29/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 2.1930 - accuracy: 0.3053 - val_loss: 2.4584 - val_accuracy: 0.2325\n",
      "Epoch 30/150\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.1877 - accuracy: 0.3128 - val_loss: 2.1890 - val_accuracy: 0.3080\n",
      "Epoch 31/150\n",
      "150/150 [==============================] - 513s 3s/step - loss: 2.2010 - accuracy: 0.3152 - val_loss: 2.3848 - val_accuracy: 0.2660\n",
      "Epoch 32/150\n",
      "150/150 [==============================] - 521s 3s/step - loss: 2.1870 - accuracy: 0.3155 - val_loss: 2.6038 - val_accuracy: 0.1940\n",
      "Epoch 33/150\n",
      "150/150 [==============================] - 519s 3s/step - loss: 2.1980 - accuracy: 0.3046 - val_loss: 2.2160 - val_accuracy: 0.3150\n",
      "Epoch 34/150\n",
      "150/150 [==============================] - 517s 3s/step - loss: 2.1700 - accuracy: 0.3170 - val_loss: 2.8630 - val_accuracy: 0.1760\n",
      "Epoch 35/150\n",
      "150/150 [==============================] - 515s 3s/step - loss: 2.1855 - accuracy: 0.3145 - val_loss: 2.1534 - val_accuracy: 0.3195\n",
      "Epoch 36/150\n",
      "150/150 [==============================] - 530s 4s/step - loss: 2.1419 - accuracy: 0.3367 - val_loss: 2.2845 - val_accuracy: 0.2965\n",
      "Epoch 37/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 2.1262 - accuracy: 0.3327 - val_loss: 2.1272 - val_accuracy: 0.3375\n",
      "Epoch 38/150\n",
      "150/150 [==============================] - 519s 3s/step - loss: 2.1807 - accuracy: 0.3172 - val_loss: 2.1391 - val_accuracy: 0.3350\n",
      "Epoch 39/150\n",
      "150/150 [==============================] - 523s 3s/step - loss: 2.1864 - accuracy: 0.3120 - val_loss: 2.4262 - val_accuracy: 0.2645\n",
      "Epoch 40/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 2.1842 - accuracy: 0.3207 - val_loss: 2.2680 - val_accuracy: 0.2810\n",
      "Epoch 41/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 2.1457 - accuracy: 0.3308 - val_loss: 2.1124 - val_accuracy: 0.3190\n",
      "Epoch 42/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 2.1404 - accuracy: 0.3282 - val_loss: 2.3455 - val_accuracy: 0.2560\n",
      "Epoch 43/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 2.1648 - accuracy: 0.3260 - val_loss: 2.1710 - val_accuracy: 0.3240\n",
      "Epoch 44/150\n",
      "150/150 [==============================] - 523s 3s/step - loss: 2.1200 - accuracy: 0.3317 - val_loss: 2.0811 - val_accuracy: 0.3485\n",
      "Epoch 45/150\n",
      "150/150 [==============================] - 528s 4s/step - loss: 2.1006 - accuracy: 0.3353 - val_loss: 2.8024 - val_accuracy: 0.2215\n",
      "Epoch 46/150\n",
      "150/150 [==============================] - 510s 3s/step - loss: 2.1020 - accuracy: 0.3357 - val_loss: 2.1424 - val_accuracy: 0.3210\n",
      "Epoch 47/150\n",
      "150/150 [==============================] - 511s 3s/step - loss: 2.0960 - accuracy: 0.3395 - val_loss: 2.2684 - val_accuracy: 0.2785\n",
      "Epoch 48/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.0929 - accuracy: 0.3454 - val_loss: 2.3031 - val_accuracy: 0.2985\n",
      "Epoch 49/150\n",
      "150/150 [==============================] - 525s 3s/step - loss: 2.0796 - accuracy: 0.3368 - val_loss: 2.0900 - val_accuracy: 0.3355\n",
      "Epoch 50/150\n",
      "150/150 [==============================] - 512s 3s/step - loss: 2.1140 - accuracy: 0.3287 - val_loss: 2.1982 - val_accuracy: 0.3270\n",
      "Epoch 51/150\n",
      "150/150 [==============================] - 514s 3s/step - loss: 2.0621 - accuracy: 0.3482 - val_loss: 2.1197 - val_accuracy: 0.3345\n",
      "Epoch 52/150\n",
      "150/150 [==============================] - 534s 4s/step - loss: 2.1075 - accuracy: 0.3377 - val_loss: 2.0786 - val_accuracy: 0.3480\n",
      "Epoch 53/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 2.0673 - accuracy: 0.3572 - val_loss: 2.0260 - val_accuracy: 0.3715\n",
      "Epoch 54/150\n",
      "150/150 [==============================] - 516s 3s/step - loss: 2.0777 - accuracy: 0.3437 - val_loss: 2.2411 - val_accuracy: 0.3000\n",
      "Epoch 55/150\n",
      "150/150 [==============================] - 517s 3s/step - loss: 2.0508 - accuracy: 0.3475 - val_loss: 2.1219 - val_accuracy: 0.3430\n",
      "Epoch 56/150\n",
      "150/150 [==============================] - 525s 3s/step - loss: 2.0700 - accuracy: 0.3487 - val_loss: 2.1365 - val_accuracy: 0.3310\n",
      "Epoch 57/150\n",
      "150/150 [==============================] - 532s 4s/step - loss: 2.0434 - accuracy: 0.3577 - val_loss: 2.1768 - val_accuracy: 0.3220\n",
      "Epoch 58/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 2.0177 - accuracy: 0.3668 - val_loss: 2.1262 - val_accuracy: 0.3405\n",
      "Epoch 59/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 2.0295 - accuracy: 0.3595 - val_loss: 2.1437 - val_accuracy: 0.3330\n",
      "Epoch 60/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 2.0523 - accuracy: 0.3625 - val_loss: 2.0793 - val_accuracy: 0.3575\n",
      "Epoch 61/150\n",
      "150/150 [==============================] - 541s 4s/step - loss: 2.1080 - accuracy: 0.3370 - val_loss: 2.1416 - val_accuracy: 0.3285\n",
      "Epoch 62/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.1553 - accuracy: 0.3300 - val_loss: 2.7053 - val_accuracy: 0.2315\n",
      "Epoch 63/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.0880 - accuracy: 0.3435 - val_loss: 2.0320 - val_accuracy: 0.3505\n",
      "Epoch 64/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 2.0716 - accuracy: 0.3492 - val_loss: 2.2017 - val_accuracy: 0.3175\n",
      "Epoch 65/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 2.0967 - accuracy: 0.3417 - val_loss: 2.1589 - val_accuracy: 0.3220\n",
      "Epoch 66/150\n",
      "150/150 [==============================] - 544s 4s/step - loss: 2.0691 - accuracy: 0.3512 - val_loss: 2.1656 - val_accuracy: 0.3235\n",
      "Epoch 67/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.0492 - accuracy: 0.3529 - val_loss: 2.0554 - val_accuracy: 0.3570\n",
      "Epoch 68/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.0151 - accuracy: 0.3673 - val_loss: 1.9930 - val_accuracy: 0.3745\n",
      "Epoch 69/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 2.0023 - accuracy: 0.3703 - val_loss: 2.1694 - val_accuracy: 0.3200\n",
      "Epoch 70/150\n",
      "150/150 [==============================] - 542s 4s/step - loss: 2.0291 - accuracy: 0.3587 - val_loss: 2.0355 - val_accuracy: 0.3505\n",
      "Epoch 71/150\n",
      "150/150 [==============================] - 541s 4s/step - loss: 2.0172 - accuracy: 0.3623 - val_loss: 1.9959 - val_accuracy: 0.3680\n",
      "Epoch 72/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 2.0202 - accuracy: 0.3645 - val_loss: 1.9636 - val_accuracy: 0.3790\n",
      "Epoch 73/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9639 - accuracy: 0.3857 - val_loss: 1.9236 - val_accuracy: 0.3925\n",
      "Epoch 74/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 2.0114 - accuracy: 0.3675 - val_loss: 1.9265 - val_accuracy: 0.4030\n",
      "Epoch 75/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.9830 - accuracy: 0.3707 - val_loss: 2.3455 - val_accuracy: 0.2500\n",
      "Epoch 76/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9847 - accuracy: 0.3715 - val_loss: 2.0841 - val_accuracy: 0.3520\n",
      "Epoch 77/150\n",
      "150/150 [==============================] - 541s 4s/step - loss: 1.9837 - accuracy: 0.3743 - val_loss: 2.1111 - val_accuracy: 0.3320\n",
      "Epoch 78/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 2.0031 - accuracy: 0.3732 - val_loss: 1.9646 - val_accuracy: 0.3870\n",
      "Epoch 79/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 1.9876 - accuracy: 0.3619 - val_loss: 2.0309 - val_accuracy: 0.3505\n",
      "Epoch 80/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.9550 - accuracy: 0.3885 - val_loss: 1.9392 - val_accuracy: 0.3980\n",
      "Epoch 81/150\n",
      "150/150 [==============================] - 541s 4s/step - loss: 1.9439 - accuracy: 0.3828 - val_loss: 2.0305 - val_accuracy: 0.3765\n",
      "Epoch 82/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 1.9600 - accuracy: 0.3770 - val_loss: 1.9545 - val_accuracy: 0.3860\n",
      "Epoch 83/150\n",
      "150/150 [==============================] - 542s 4s/step - loss: 1.9311 - accuracy: 0.3868 - val_loss: 1.9917 - val_accuracy: 0.3745\n",
      "Epoch 84/150\n",
      "150/150 [==============================] - 536s 4s/step - loss: 1.9767 - accuracy: 0.3830 - val_loss: 2.0092 - val_accuracy: 0.3635\n",
      "Epoch 85/150\n",
      "150/150 [==============================] - 542s 4s/step - loss: 1.9459 - accuracy: 0.3843 - val_loss: 1.9300 - val_accuracy: 0.3920\n",
      "Epoch 86/150\n",
      "150/150 [==============================] - 543s 4s/step - loss: 1.9333 - accuracy: 0.3888 - val_loss: 1.9175 - val_accuracy: 0.3865\n",
      "Epoch 87/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9271 - accuracy: 0.3950 - val_loss: 1.8951 - val_accuracy: 0.4015\n",
      "Epoch 88/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.9167 - accuracy: 0.4082 - val_loss: 1.8448 - val_accuracy: 0.4130\n",
      "Epoch 89/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.9145 - accuracy: 0.4013 - val_loss: 1.9061 - val_accuracy: 0.4060\n",
      "Epoch 90/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 1.9508 - accuracy: 0.3902 - val_loss: 2.0141 - val_accuracy: 0.3755\n",
      "Epoch 91/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.9411 - accuracy: 0.3823 - val_loss: 1.9000 - val_accuracy: 0.3945\n",
      "Epoch 92/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9686 - accuracy: 0.3772 - val_loss: 1.9273 - val_accuracy: 0.3910\n",
      "Epoch 93/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9131 - accuracy: 0.4021 - val_loss: 1.9525 - val_accuracy: 0.3955\n",
      "Epoch 94/150\n",
      "150/150 [==============================] - 540s 4s/step - loss: 1.9015 - accuracy: 0.3907 - val_loss: 1.9563 - val_accuracy: 0.3960\n",
      "Epoch 95/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 1.8856 - accuracy: 0.4092 - val_loss: 1.9038 - val_accuracy: 0.4025\n",
      "Epoch 96/150\n",
      "150/150 [==============================] - 538s 4s/step - loss: 1.8986 - accuracy: 0.4003 - val_loss: 1.8299 - val_accuracy: 0.4170\n",
      "Epoch 97/150\n",
      "150/150 [==============================] - 542s 4s/step - loss: 1.8989 - accuracy: 0.3995 - val_loss: 1.9598 - val_accuracy: 0.3910\n",
      "Epoch 98/150\n",
      "150/150 [==============================] - 539s 4s/step - loss: 1.9219 - accuracy: 0.3992 - val_loss: 1.8781 - val_accuracy: 0.4175\n",
      "Epoch 99/150\n",
      "150/150 [==============================] - 533s 4s/step - loss: 1.8932 - accuracy: 0.4022 - val_loss: 1.9257 - val_accuracy: 0.4010\n",
      "Epoch 100/150\n",
      "150/150 [==============================] - 521s 3s/step - loss: 1.9011 - accuracy: 0.4057 - val_loss: 1.8907 - val_accuracy: 0.4155\n",
      "Epoch 101/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 1.8969 - accuracy: 0.4042 - val_loss: 1.8948 - val_accuracy: 0.3960\n",
      "Epoch 102/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 1.8883 - accuracy: 0.4077 - val_loss: 1.8669 - val_accuracy: 0.4195\n",
      "Epoch 103/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 1.8665 - accuracy: 0.4132 - val_loss: 1.9999 - val_accuracy: 0.3795\n",
      "Epoch 104/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 1.9104 - accuracy: 0.3988 - val_loss: 1.8355 - val_accuracy: 0.4180\n",
      "Epoch 105/150\n",
      "150/150 [==============================] - 523s 3s/step - loss: 1.8710 - accuracy: 0.4152 - val_loss: 1.9147 - val_accuracy: 0.4080\n",
      "Epoch 106/150\n",
      "150/150 [==============================] - 524s 3s/step - loss: 1.8761 - accuracy: 0.4127 - val_loss: 1.7979 - val_accuracy: 0.4395\n",
      "Epoch 107/150\n",
      "150/150 [==============================] - 523s 3s/step - loss: 1.8853 - accuracy: 0.4057 - val_loss: 1.9606 - val_accuracy: 0.3800\n",
      "Epoch 108/150\n",
      "150/150 [==============================] - 518s 3s/step - loss: 1.8608 - accuracy: 0.4172 - val_loss: 2.1959 - val_accuracy: 0.3285\n",
      "Epoch 109/150\n",
      "150/150 [==============================] - 516s 3s/step - loss: 1.8638 - accuracy: 0.4172 - val_loss: 1.8818 - val_accuracy: 0.4035\n",
      "Epoch 110/150\n",
      "150/150 [==============================] - 514s 3s/step - loss: 1.8609 - accuracy: 0.4242 - val_loss: 1.8944 - val_accuracy: 0.4050\n",
      "Epoch 111/150\n",
      "150/150 [==============================] - 514s 3s/step - loss: 1.8347 - accuracy: 0.4285 - val_loss: 1.8985 - val_accuracy: 0.4080\n",
      "Epoch 112/150\n",
      "150/150 [==============================] - 515s 3s/step - loss: 1.8385 - accuracy: 0.4152 - val_loss: 1.7804 - val_accuracy: 0.4485\n",
      "Epoch 113/150\n",
      "150/150 [==============================] - 512s 3s/step - loss: 1.8551 - accuracy: 0.4155 - val_loss: 1.7568 - val_accuracy: 0.4520\n",
      "Epoch 114/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 1.8452 - accuracy: 0.4305 - val_loss: 1.8013 - val_accuracy: 0.4385\n",
      "Epoch 115/150\n",
      "150/150 [==============================] - 526s 4s/step - loss: 1.8621 - accuracy: 0.4197 - val_loss: 1.7323 - val_accuracy: 0.4750\n",
      "Epoch 116/150\n",
      "150/150 [==============================] - 513s 3s/step - loss: 1.8305 - accuracy: 0.4275 - val_loss: 1.7359 - val_accuracy: 0.4595\n",
      "Epoch 117/150\n",
      "150/150 [==============================] - 530s 4s/step - loss: 1.8291 - accuracy: 0.4220 - val_loss: 1.8120 - val_accuracy: 0.4415\n",
      "Epoch 118/150\n",
      "150/150 [==============================] - 546s 4s/step - loss: 1.8765 - accuracy: 0.4098 - val_loss: 2.8851 - val_accuracy: 0.1375\n",
      "Epoch 119/150\n",
      "150/150 [==============================] - 537s 4s/step - loss: 1.9418 - accuracy: 0.3908 - val_loss: 1.8125 - val_accuracy: 0.4365\n",
      "Epoch 120/150\n",
      "150/150 [==============================] - 531s 4s/step - loss: 1.8668 - accuracy: 0.4173 - val_loss: 1.8882 - val_accuracy: 0.4020\n",
      "Epoch 121/150\n",
      "150/150 [==============================] - 519s 3s/step - loss: 1.8615 - accuracy: 0.4133 - val_loss: 1.9163 - val_accuracy: 0.3980\n",
      "Epoch 122/150\n",
      "150/150 [==============================] - 522s 3s/step - loss: 1.8475 - accuracy: 0.4307 - val_loss: 1.9828 - val_accuracy: 0.3840\n",
      "Epoch 123/150\n",
      "150/150 [==============================] - 518s 3s/step - loss: 1.8328 - accuracy: 0.4270 - val_loss: 1.7410 - val_accuracy: 0.4440\n",
      "Epoch 124/150\n",
      "150/150 [==============================] - 529s 4s/step - loss: 1.8160 - accuracy: 0.4275 - val_loss: 1.8319 - val_accuracy: 0.4235\n",
      "Epoch 125/150\n",
      "150/150 [==============================] - 533s 4s/step - loss: 1.8375 - accuracy: 0.4205 - val_loss: 1.9484 - val_accuracy: 0.4025\n",
      "Epoch 126/150\n",
      "150/150 [==============================] - 568s 4s/step - loss: 1.8329 - accuracy: 0.4253 - val_loss: 2.0699 - val_accuracy: 0.3675\n",
      "Epoch 127/150\n",
      "150/150 [==============================] - 546s 4s/step - loss: 1.8190 - accuracy: 0.4240 - val_loss: 1.7732 - val_accuracy: 0.4640\n",
      "Epoch 128/150\n",
      "150/150 [==============================] - 526s 4s/step - loss: 1.8204 - accuracy: 0.4288 - val_loss: 1.8641 - val_accuracy: 0.4250\n",
      "Epoch 129/150\n",
      "150/150 [==============================] - 525s 4s/step - loss: 1.7859 - accuracy: 0.4363 - val_loss: 1.8072 - val_accuracy: 0.4320\n",
      "Epoch 130/150\n",
      "150/150 [==============================] - 533s 4s/step - loss: 1.7513 - accuracy: 0.4533 - val_loss: 1.7554 - val_accuracy: 0.4425\n",
      "Epoch 131/150\n",
      "150/150 [==============================] - 531s 4s/step - loss: 1.7569 - accuracy: 0.4545 - val_loss: 2.1251 - val_accuracy: 0.3375\n",
      "Epoch 132/150\n",
      "150/150 [==============================] - 508s 3s/step - loss: 1.7539 - accuracy: 0.4523 - val_loss: 1.7168 - val_accuracy: 0.4655\n",
      "Epoch 133/150\n",
      "150/150 [==============================] - 519s 3s/step - loss: 1.7892 - accuracy: 0.4433 - val_loss: 1.8394 - val_accuracy: 0.4360\n",
      "Epoch 134/150\n",
      "150/150 [==============================] - 499s 3s/step - loss: 1.7837 - accuracy: 0.4378 - val_loss: 1.8460 - val_accuracy: 0.4560\n",
      "Epoch 135/150\n",
      "150/150 [==============================] - 504s 3s/step - loss: 1.7999 - accuracy: 0.4298 - val_loss: 1.7255 - val_accuracy: 0.4590\n",
      "Epoch 136/150\n",
      "150/150 [==============================] - 509s 3s/step - loss: 1.7500 - accuracy: 0.4532 - val_loss: 1.8174 - val_accuracy: 0.4245\n",
      "Epoch 137/150\n",
      "150/150 [==============================] - 529s 4s/step - loss: 1.7560 - accuracy: 0.4495 - val_loss: 1.7732 - val_accuracy: 0.4485\n",
      "Epoch 138/150\n",
      "150/150 [==============================] - 528s 4s/step - loss: 1.7816 - accuracy: 0.4462 - val_loss: 1.7893 - val_accuracy: 0.4355\n",
      "Epoch 139/150\n",
      "150/150 [==============================] - 559s 4s/step - loss: 1.7479 - accuracy: 0.4457 - val_loss: 1.8954 - val_accuracy: 0.4050\n",
      "Epoch 140/150\n",
      "150/150 [==============================] - 490s 3s/step - loss: 1.7797 - accuracy: 0.4517 - val_loss: 2.2786 - val_accuracy: 0.2885\n",
      "Epoch 141/150\n",
      "150/150 [==============================] - 490s 3s/step - loss: 1.8081 - accuracy: 0.4397 - val_loss: 1.8857 - val_accuracy: 0.4060\n",
      "Epoch 142/150\n",
      "150/150 [==============================] - 488s 3s/step - loss: 1.7580 - accuracy: 0.4583 - val_loss: 1.8044 - val_accuracy: 0.4430\n",
      "Epoch 143/150\n",
      "150/150 [==============================] - 490s 3s/step - loss: 1.7794 - accuracy: 0.4342 - val_loss: 1.7468 - val_accuracy: 0.4455\n",
      "Epoch 144/150\n",
      "150/150 [==============================] - 495s 3s/step - loss: 1.7883 - accuracy: 0.4367 - val_loss: 1.7924 - val_accuracy: 0.4425\n",
      "Epoch 145/150\n",
      "150/150 [==============================] - 493s 3s/step - loss: 1.7735 - accuracy: 0.4394 - val_loss: 1.7711 - val_accuracy: 0.4575\n",
      "Epoch 146/150\n",
      "150/150 [==============================] - 492s 3s/step - loss: 1.7711 - accuracy: 0.4440 - val_loss: 1.6713 - val_accuracy: 0.4820\n",
      "Epoch 147/150\n",
      "150/150 [==============================] - 500s 3s/step - loss: 1.7736 - accuracy: 0.4553 - val_loss: 1.7596 - val_accuracy: 0.4470\n",
      "Epoch 148/150\n",
      "150/150 [==============================] - 521s 3s/step - loss: 1.7670 - accuracy: 0.4437 - val_loss: 1.7260 - val_accuracy: 0.4580\n",
      "Epoch 149/150\n",
      "150/150 [==============================] - 521s 3s/step - loss: 1.7154 - accuracy: 0.4594 - val_loss: 1.6854 - val_accuracy: 0.4660\n",
      "Epoch 150/150\n",
      "150/150 [==============================] - 520s 3s/step - loss: 1.7078 - accuracy: 0.4550 - val_loss: 1.9749 - val_accuracy: 0.3865\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1de4a8db160>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# fig_generator\n",
    "model3.fit_generator(\n",
    "    generator,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=300,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 150, 150, 32)      896       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 75, 75, 32)        0         \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 75, 75, 32)        128       \n_________________________________________________________________\ndropout (Dropout)            (None, 75, 75, 32)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 75, 75, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 37, 37, 64)        0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 37, 37, 64)        256       \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 37, 37, 64)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 37, 37, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 18, 18, 128)       0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 18, 18, 128)       512       \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 18, 18, 128)       0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 18, 18, 256)       295168    \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 9, 9, 256)         0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 9, 9, 256)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 20736)             0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               5308672   \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 256)               1024      \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 17)                4369      \n=================================================================\nTotal params: 5,703,377\nTrainable params: 5,702,417\nNon-trainable params: 960\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=load_model('model2.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "img=image.load_img('pokemon-test-image\\water\\야돈_3.jpg', target_size=(150, 150))\n",
    "type(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image.load_img('pokemon-test-image/fire/윈디_120.jpg', target_size=(150, 150))\n",
    "x = image.img_to_array(x)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "x = x/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 150, 150, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 243
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.01801904, 0.00284082, 0.01502389, 0.00885601, 0.03310708,\n",
       "       0.02035785, 0.04623378, 0.2191215 , 0.10316587, 0.01619275,\n",
       "       0.00770238, 0.04910393, 0.3172349 , 0.00828799, 0.11408798,\n",
       "       0.00686288, 0.01380135], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "predict=model.predict(x).flatten()\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.2191215, 0.3172349], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 197
    }
   ],
   "source": [
    "np.sort(predict)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 1, 15, 10, 13,  3, 16,  2,  9,  0,  5,  4,  6, 11,  8, 14,  7, 12],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "s=np.argsort(predict)\n",
    "p1=np.argmax(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'노멀': array([31.7], dtype=float32)}"
      ]
     },
     "metadata": {},
     "execution_count": 250
    }
   ],
   "source": [
    "{categories[p1]: 100*np.sort(predict)[-1:].round(3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1, 15, 10, 13,  3, 16,  2,  9,  0,  5,  4,  6, 11,  8, 14,  7,\n",
       "        12]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 153
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000021A8F5D7510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'노멀': 26.14}"
      ]
     },
     "metadata": {},
     "execution_count": 264
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from PIL import Image\n",
    "\n",
    "pokemon_model=load_model('model2.h5')\n",
    "\n",
    "# categories=['bug', 'dragon', 'electricity', 'esp', 'evil', 'fairy', 'fight', 'fire', 'ghost', 'grass', 'ice', 'land', 'normal', 'posion', 'rock', 'steel', 'water']\n",
    "categories=['벌레', '드래곤', '전기', '에스퍼', '악', '요정', '격투', '불꽃', '고스트', '풀', '얼음', '땅', '노멀', '독', '바위', '강철', '물']\n",
    "\n",
    "# file = request.files['original-file']     #파일 업로드\n",
    "# filename = file.filename   #파일 읽기\n",
    "# absolute_path = os.path.join('images', filename)   #경로를 병합하여 새 경로 생성\n",
    "absolute_path = image.load_img('pokemon-test-image/fire/윈디_120.jpg')\n",
    "#  ,target_size=(150,150))\n",
    "\n",
    "# file.save(absolute_path)  # 업로드된 파일 저장\n",
    "\n",
    "absolute_path = absolute_path.resize((150, 150))\n",
    "x = image.img_to_array(absolute_path)\n",
    "# x = x.resize((1,150,150,3))\n",
    "x = x.reshape((1,) + x.shape)\n",
    "x = x/255.\n",
    "\n",
    "# predict=pokemon_model.predict(x)\n",
    "# result=np.argmax(predict)\n",
    "# result=categories[result]\n",
    "\n",
    "predict=pokemon_model.predict(x)\n",
    "result=np.argmax(predict)\n",
    "result=categories[np.argmax(predict)]\n",
    "\n",
    "result={result: (100*np.max(predict)).round(2)}\n",
    "# np.max(predict).round(2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}